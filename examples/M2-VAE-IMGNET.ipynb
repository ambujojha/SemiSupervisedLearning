{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sys.path.append(\"../semi-supervised\")\n",
    "\n",
    "from functools import reduce\n",
    "from operator import __or__\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA: {}\".format(cuda))\n",
    "\n",
    "def binary_cross_entropy(r, x):\n",
    "    \"Drop in replacement until PyTorch adds `reduce` keyword.\"\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "n_labels = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225),\n",
    "                         inplace=True)\n",
    "])\n",
    "\n",
    "dataset1 = datasets.ImageFolder('/scratch/apo249/ssl_data_96/supervised/train/',\n",
    "                                transform=transformations)\n",
    "\n",
    "dataset2 = datasets.ImageFolder('/scratch/apo249/ssl_data_96/unsupervised/',\n",
    "                                transform=transformations)\n",
    "\n",
    "dataset3 = datasets.ImageFolder('/scratch/apo249/ssl_data_96/supervised/val/',\n",
    "                                transform=transformations)\n",
    "\n",
    "\n",
    "#dataset = data.ConcatDataset((dataset1, dataset2))\n",
    "\n",
    "#train_ratio = 0.9\n",
    "#train_set_size = int(train_ratio * len(dataset))\n",
    "#val_set_size = len(dataset) - train_set_size\n",
    "\n",
    "#train_data, val_data = data.random_split(dataset,\n",
    "#                                         (train_set_size, val_set_size))\n",
    "\n",
    "train_lab_loader = data.DataLoader(dataset1, batch_size=128, shuffle=True)\n",
    "train_unlab_loader = data.DataLoader(dataset2, batch_size=128, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset3, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "4000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lab_loader))\n",
    "print(len(train_unlab_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import GaussianSample, GaussianMerge, GumbelSoftmax\n",
    "from inference import log_gaussian, log_standard_gaussian\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dims, sample_layer=GaussianSample):\n",
    "        \"\"\"\n",
    "        Inference network\n",
    "\n",
    "        Attempts to infer the probability distribution\n",
    "        p(z|x) from the data by fitting a variational\n",
    "        distribution q_φ(z|x). Returns the two parameters\n",
    "        of the distribution (µ, log σ²).\n",
    "\n",
    "        :param dims: dimensions of the networks\n",
    "           given by the number of neurons on the form\n",
    "           [input_dim, [hidden_dims], latent_dim].\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        [x_dim, h_dim, z_dim] = dims\n",
    "        neurons = [x_dim, *h_dim]\n",
    "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
    "\n",
    "        self.hidden = nn.ModuleList(linear_layers)\n",
    "        self.sample = sample_layer(h_dim[-1], z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.sample(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        \"\"\"\n",
    "        Generative network\n",
    "\n",
    "        Generates samples from the original distribution\n",
    "        p(x) by transforming a latent representation, e.g.\n",
    "        by finding p_θ(x|z).\n",
    "\n",
    "        :param dims: dimensions of the networks\n",
    "            given by the number of neurons on the form\n",
    "            [latent_dim, [hidden_dims], input_dim].\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        [z_dim, h_dim, x_dim] = dims\n",
    "\n",
    "        neurons = [z_dim, *h_dim]\n",
    "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n",
    "        self.hidden = nn.ModuleList(linear_layers)\n",
    "\n",
    "        self.reconstruction = nn.Linear(h_dim[-1], x_dim)\n",
    "\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.output_activation(self.reconstruction(x))\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        \"\"\"\n",
    "        Single hidden layer classifier\n",
    "        with softmax output.\n",
    "        \"\"\"\n",
    "        super(Classifier, self).__init__()\n",
    "        [x_dim, h_dim, y_dim] = dims\n",
    "        self.dense = nn.Linear(x_dim, h_dim)\n",
    "        self.logits = nn.Linear(h_dim, y_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense(x))\n",
    "        x = F.softmax(self.logits(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VariationalAutoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-624a28860ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDeepGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariationalAutoencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \"\"\"\n\u001b[1;32m      4\u001b[0m         \u001b[0mM2\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mreplication\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpaper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'Semi-Supervised Learning with Deep Generative Models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VariationalAutoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "class DeepGenerativeModel(VariationalAutoencoder):\n",
    "    def __init__(self, dims):\n",
    "        \"\"\"\n",
    "        M2 code replication from the paper\n",
    "        'Semi-Supervised Learning with Deep Generative Models'\n",
    "        (Kingma 2014) in PyTorch.\n",
    "\n",
    "        The \"Generative semi-supervised model\" is a probabilistic\n",
    "        model that incorporates label information in both\n",
    "        inference and generation.\n",
    "\n",
    "        Initialise a new generative model\n",
    "        :param dims: dimensions of x, y, z and hidden layers.\n",
    "        \"\"\"\n",
    "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
    "        super(DeepGenerativeModel, self).__init__([x_dim, z_dim, h_dim])\n",
    "\n",
    "        self.encoder = Encoder([x_dim + self.y_dim, h_dim, z_dim])\n",
    "        self.decoder = Decoder([z_dim + self.y_dim, list(reversed(h_dim)), x_dim])\n",
    "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Add label and data and generate latent variable\n",
    "        z, z_mu, z_log_var = self.encoder(torch.cat([x, y], dim=1))\n",
    "\n",
    "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
    "\n",
    "        # Reconstruct data point from latent data and label\n",
    "        x_mu = self.decoder(torch.cat([z, y], dim=1))\n",
    "\n",
    "        return x_mu\n",
    "\n",
    "    def classify(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, z, y):\n",
    "        \"\"\"\n",
    "        Samples from the Decoder to generate an x.\n",
    "        :param z: latent normal variable\n",
    "        :param y: label (one-hot encoded)\n",
    "        :return: x\n",
    "        \"\"\"\n",
    "        y = y.float()\n",
    "        x = self.decoder(torch.cat([z, y], dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../semi-supervised/models/vae.py:114: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight.data)\n",
      "../semi-supervised/models/dgm.py:50: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight.data)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9546b4c06e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/apo249/semi-supervised-pytorch/semi-supervised/inference/variational.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Increase sampling dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/apo249/semi-supervised-pytorch/semi-supervised/inference/variational.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from itertools import repeat, cycle\n",
    "    from torch.autograd import Variable\n",
    "    from inference import SVI, DeterministicWarmup, ImportanceWeightedSampler\n",
    "\n",
    "    #labelled, unlabelled, validation = get_imgnet(location=\"./\", batch_size=100, labels_per_class=10)\n",
    "    labelled = train_lab_loader\n",
    "    unlabelled = train_unlab_loader\n",
    "    validation = val_loader\n",
    "    \n",
    "    alpha = 0.1 * len(unlabelled) / len(labelled)\n",
    "\n",
    "    models = []\n",
    "\n",
    "    # Kingma 2014, M2 model. Reported: 88%, achieved: ??%\n",
    "    from models import DeepGenerativeModel\n",
    "    models += [DeepGenerativeModel([96*96, n_labels, 50, [600, 600]])]\n",
    "\n",
    "    # Maaløe 2016, ADGM model. Reported: 99.4%, achieved: ??%\n",
    "    # from models import AuxiliaryDeepGenerativeModel\n",
    "    # models += [AuxiliaryDeepGenerativeModel([784, n_labels, 100, 100, [500, 500]])]\n",
    "\n",
    "    # from models import LadderDeepGenerativeModel\n",
    "    # models += [LadderDeepGenerativeModel([784, n_labels, [32, 16, 8], [128, 128, 128]])]\n",
    "\n",
    "    for model in models:\n",
    "        if cuda: model = model.cuda()\n",
    "\n",
    "        beta = DeterministicWarmup(n=4*len(unlabelled)*100)\n",
    "        sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
    "\n",
    "        elbo = SVI(model, likelihood=binary_cross_entropy, beta=beta, sampler=sampler)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
    "\n",
    "        epochs = 251\n",
    "        best = 0.0\n",
    "\n",
    "        file = open(model.__class__.__name__ + \".log\", 'w+')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss, labelled_loss, unlabelled_loss, accuracy = (0, 0, 0, 0)\n",
    "            for (x, y), (u, _) in zip(cycle(labelled), unlabelled):\n",
    "                # Wrap in variables\n",
    "                x, y, u = Variable(x), Variable(y), Variable(u)\n",
    "\n",
    "                if cuda:\n",
    "                    # They need to be on the same device and be synchronized.\n",
    "                    x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "                    u = u.cuda(device=0)\n",
    "\n",
    "                L = -elbo(x, y)\n",
    "                U = -elbo(u)\n",
    "\n",
    "                # Add auxiliary classification loss q(y|x)\n",
    "                logits = model.classify(x)\n",
    "                classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "                J_alpha = L - alpha * classication_loss + U\n",
    "\n",
    "                J_alpha.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                total_loss += J_alpha.data.item()\n",
    "                labelled_loss += L.data.item()\n",
    "                unlabelled_loss += U.data.item()\n",
    "\n",
    "                _, pred_idx = torch.max(logits, 1)\n",
    "                _, lab_idx = torch.max(y, 1)\n",
    "                accuracy += torch.mean((pred_idx.data == lab_idx.data).float())\n",
    "\n",
    "            m = len(unlabelled)\n",
    "            print(*(total_loss / m, labelled_loss / m, unlabelled_loss / m, accuracy / m), sep=\"\\t\", file=file)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                model.eval()\n",
    "                print(\"Epoch: {}\".format(epoch))\n",
    "                print(\"[Train]\\t\\t J_a: {:.2f}, L: {:.2f}, U: {:.2f}, accuracy: {:.2f}\".format(total_loss / m,\n",
    "                                                                                              labelled_loss / m,\n",
    "                                                                                              unlabelled_loss / m,\n",
    "                                                                                              accuracy / m))\n",
    "\n",
    "                total_loss, labelled_loss, unlabelled_loss, accuracy = (0, 0, 0, 0)\n",
    "                for x, y in validation:\n",
    "                    x, y = Variable(x), Variable(y)\n",
    "\n",
    "                    if cuda:\n",
    "                        x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "                    L = -elbo(x, y)\n",
    "                    U = -elbo(x)\n",
    "\n",
    "                    logits = model.classify(x)\n",
    "                    classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "                    J_alpha = L + alpha * classication_loss + U\n",
    "\n",
    "                    total_loss += J_alpha.data.item()\n",
    "                    labelled_loss += L.data.item()\n",
    "                    unlabelled_loss += U.data.item()\n",
    "\n",
    "                    _, pred_idx = torch.max(logits, 1)\n",
    "                    _, lab_idx = torch.max(y, 1)\n",
    "                    accuracy += torch.mean((pred_idx.data == lab_idx.data).float())\n",
    "\n",
    "                m = len(validation)\n",
    "                print(*(total_loss / m, labelled_loss / m, unlabelled_loss / m, accuracy / m), sep=\"\\t\", file=file)\n",
    "                print(\"[Validation]\\t J_a: {:.2f}, L: {:.2f}, U: {:.2f}, accuracy: {:.2f}\".format(total_loss / m,\n",
    "                                                                                              labelled_loss / m,\n",
    "                                                                                              unlabelled_loss / m,\n",
    "                                                                                              accuracy / m))\n",
    "\n",
    "            if accuracy > best:\n",
    "                best = accuracy\n",
    "                torch.save(model, '{}.pt'.format(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
